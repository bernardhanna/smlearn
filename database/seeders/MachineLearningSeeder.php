<?php

namespace Database\Seeders;

use Illuminate\Database\Seeder;
use Illuminate\Support\Facades\DB;
use Carbon\Carbon;

class MachineLearningSeeder extends Seeder
{
    /**
     * Run the database seeds.
     */
    public function run(): void
    {
        // Insert the MachineLearning course
        $courseId = DB::table('courses')->insertGetId([
            'slug' => 'machinelearning',
            'course_name' => 'MachineLearning',
            'description' => 'MachineLearning Course Description',
            'difficulty' => 'Beginner',
            'category' => 'Programming',
            'created_at' => Carbon::now(),
            'updated_at' => Carbon::now(),
        ]);

        // Define topics for MachineLearning course
        $topics = [
            ['course_id' => $courseId, 'name' => 'Basic'],
            // Add more topics for MachineLearning course here
        ];

        // Insert topics for MachineLearning course
        foreach ($topics as $topic) {
            $topicId = DB::table('topics')->insertGetId([
                'course_id' => $topic['course_id'],
                'name' => $topic['name'],
                'created_at' => Carbon::now(),
                'updated_at' => Carbon::now(),
            ]);

            // Define questions for each topic
            $questions = [
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the three main tasks in machine learning?',
                    'answer' => 'The three main tasks in machine learning are supervised learning, unsupervised learning, and semi-supervised learning.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How does supervised learning differ from unsupervised learning?',
                    'answer' => 'Supervised learning uses labeled data to train models, while unsupervised learning identifies patterns in unlabeled data without direct input on the output.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the goal of supervised learning?',
                    'answer' => 'The goal of supervised learning is to learn from labeled data, adjusting the model\'s parameters to minimize the loss between the model\'s prediction and the actual label.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What objectives do models in unsupervised learning typically pursue?',
                    'answer' => 'Models in unsupervised learning typically pursue objectives such as grouping data in meaningful ways (clustering) or reducing data complexity while preserving essential features (dimension reduction).',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How does semi-supervised learning work?',
                    'answer' => 'Semi-supervised learning combines supervised learning with unsupervised learning by using both labeled and unlabeled data, often controlled by a weighting factor to balance the two learning approaches.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is a loss function in supervised learning?',
                    'answer' => 'A loss function in supervised learning quantifies the difference between the model\'s prediction and the actual label, guiding the model\'s parameter adjustment to minimize this loss.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'Why might a model use semi-supervised learning?',
                    'answer' => 'A model might use semi-supervised learning when only a portion of the data is labeled, allowing it to learn from both the structure found in unlabeled data and the direct feedback from labeled data.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the significance of a loss function in machine learning models?',
                    'answer' => 'The loss function is significant in machine learning models as it reflects the learning objective and is optimized to achieve tasks like matching labels, discovering patterns, or a mix of both.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is a machine learning model from a software engineering perspective?',
                    'answer' => 'A machine learning model is akin to a computer programming function that maps inputs (arguments) to outputs (return value), describing a system or process.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'When is machine learning particularly useful?',
                    'answer' => 'Machine learning shows its strength when dealing with unstructured data like images, audio, and text, where it is difficult to design explicit logic for accurate predictions.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is symbolic regression in machine learning?',
                    'answer' => 'Symbolic regression is the task of finding the structure of a function based on provided input-output pairs, often utilizing methods like genetic programming and reinforcement learning.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the challenges in function space exploration in machine learning?',
                    'answer' => 'Exploring function space involves not only recovering parameters but also understanding the function\'s underlying structure, requiring searching for both the right formula and optimal variable values.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How does the Taylor Series relate to machine learning?',
                    'answer' => 'The Taylor Series suggests that any differentiable function can be expanded into a polynomial form, allowing polynomial regression to serve as a universal approximator in machine learning.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the limitations of polynomial modeling in machine learning?',
                    'answer' => 'Polynomial models can produce extreme values and become computationally demanding with increased complexity and cross terms, especially with high-order polynomials and multiple variables.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What basic assumption is crucial for modeling neural networks as logic gates?',
                    'answer' => 'The inputs and outputs of the neural network are limited to 0 or 1.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How can neural networks support any logical function?',
                    'answer' => 'By demonstrating that neural networks can implement basic logic gates like NOT and OR, from which AND gates and other complex logic modules can be built.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the key components to verify for constructing logic gates via neural networks?',
                    'answer' => 'Verifying whether there are weights (w) and biases (b) that satisfy the conditions for OR, NOT, and bypass gates.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How is an OR gate implemented using a neural network?',
                    'answer' => 'By constructing a neuron model with specific weights and biases to satisfy the conditions of an OR gate operation.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How can complex logical judgments be formed by combining neurons?',
                    'answer' => 'By manually structuring the neural network to combine basic logic gates like OR, NOT, and bypass to implement more complex logic operations.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the significance of neural network logic in the context of fuzzy logic?',
                    'answer' => 'Neural network logic operations, with real number inputs and outputs, are more akin to fuzzy logic rather than strict Boolean logic.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is a perceptron?',
                    'answer' => 'A perceptron, or single layer perceptron, is a simple machine learning model from the 1950s and 1960s used for binary classification, similar to logistic regression.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the core components of perceptrons that are identical to modern neural networks?',
                    'answer' => 'The core structural components of perceptrons that are identical to modern neural networks are linear transformations and activation functions.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How does the original perceptron generate a prediction?',
                    'answer' => 'The original perceptron generates a prediction using a step function that assigns a value of 1 to inputs greater than zero, and a value of 0 to inputs less than or equal to zero, reflecting the classification of the input data.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the main differences between logistic regression and the original perceptron?',
                    'answer' => 'The main differences are that logistic regression uses the Sigmoid function as its activation function and the negative log likelihood as its loss function, in contrast to the original perceptron’s step function and simple binary loss.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is a Multi-layer Perceptron (MLP)?',
                    'answer' => 'A Multi-layer Perceptron (MLP) is a structured network that includes one or more hidden layers in addition to the input and output layers, allowing it to model more complex data patterns.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How do modern deep neural networks relate to the concept of an MLP?',
                    'answer' => 'Modern deep neural networks are sophisticated versions of the MLP, with the "deep" aspect referring to the addition of more hidden layers, enabling the network to realize high-level data abstractions and model complex relationships.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the goal of training in supervised learning?',
                    'answer' => 'The goal of training in supervised learning is to reduce the disparity between the model\'s outputs and the given labels.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the three core elements in machine learning training?',
                    'answer' => 'The three core elements in machine learning training are the model, the loss function, and the optimization process.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How is training in neural networks often described?',
                    'answer' => 'Training in neural networks, utilizing backpropagation, is often described as a process of trial and error involving the model, loss, and optimization.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What does the loss function measure in machine learning training?',
                    'answer' => 'The loss function measures the loss value between the model’s output and the actual label, serving as a measurement for the model\'s prediction accuracy.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the role of the optimization algorithm in machine learning training?',
                    'answer' => 'The optimization algorithm interprets the feedback from the loss function to identify the causes of the current level of loss, guides modifications to improve the model, and thereby reduces the loss.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What does the training process involve according to the provided pseudocode?',
                    'answer' => 'According to the provided pseudocode, the training process involves looping over the dataset multiple times, calculating the model\'s output, computing the loss value, adjusting the model based on the loss, and repeating until optimal parameters are found.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What are the three stages in a machine learning workflow?',
                    'answer' => 'The three stages in a machine learning workflow are training, validation, and testing.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is K-fold cross-validation?',
                    'answer' => 'K-fold cross-validation is a method where the data is divided into K equal parts, or folds, with each fold serving as the validation set once and as part of the training set K-1 times. The average result of the K trials is used as the overall result of the model.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the purpose of the validation phase in machine learning?',
                    'answer' => 'The purpose of the validation phase in machine learning is to search for the best configuration by adjusting hyperparameters, model structures, loss functions, and optimization algorithms to enhance the task-specific score or metric.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How does testing differ from validation in machine learning?',
                    'answer' => 'Testing differs from validation in that it uses a separate set of unseen data for the final evaluation of the model\'s performance, whereas validation is an ongoing process during model development using data that may subtly influence configurations.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'Why is K-fold cross-validation beneficial in machine learning?',
                    'answer' => 'K-fold cross-validation is beneficial because it ensures that every observation from the original dataset has the chance of appearing in both the training and test set, which is particularly useful when the dataset is not too large.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What challenges does the validation phase address in machine learning?',
                    'answer' => 'The validation phase addresses the challenge of overfitting and the simplification of loss functions that might not well reflect the requirements of the target task, potentially diminishing the model\'s performance on unseen data.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'Why is the testing phase considered the final evaluation in machine learning?',
                    'answer' => 'The testing phase is considered the final evaluation because it measures the model\'s performance on a separate set of unseen data, providing a true assessment of its effectiveness and generalization capability beyond the data used for training and validation.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'Why are deep neural networks preferred over shallow and wide ones?',
                    'answer' => 'Deep neural networks are preferred because they can structurally analyze data and extract structured patterns more effectively than shallow networks, allowing for more intricate logic encapsulation and data analysis.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How does the concept of depth in neural networks relate to code abstraction?',
                    'answer' => 'The concept of depth in neural networks relates to code abstraction by allowing the reuse of fundamental sub-functions (akin to encapsulated logic in code) across different layers, simplifying complexity and enabling more complex data analysis.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the problem with shallow neural network structures?',
                    'answer' => 'Shallow neural network structures face the problem of redundant neuron proliferation due to the absence of a hierarchical structure, leading to inefficient learning and wasted computational resources.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How do deep neural networks recognize complex patterns such as a cat?',
                    'answer' => 'Deep neural networks recognize complex patterns by initially learning to recognize fundamental patterns like edges and lines in the initial layers, and subsequently combining these patterns to identify more complex shapes and features in higher layers.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'How do the initial and later layers of a CNN differ in terms of pattern recognition?',
                    'answer' => 'In a CNN, the initial layers focus on extracting fundamental patterns such as edges, while later layers identify more complex and specific patterns, using a combination of logic gates to classify inputs based on detected and undetected patterns.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                ],
                [
                    'topic_id' => $topicId,
                    'question' => 'What is the main difference between Batch Gradient Descent and Stochastic Gradient Descent?',
                    'answer' => 'Batch Gradient Descent calculates the gradient using all samples in the dataset, while Stochastic Gradient Descent uses a subset or a single sample, introducing randomness that can help escape suboptimal local minima.',
                    'difficulty' => 'Easy',
                    'type' => 'speech',
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'Why is randomness introduced in Stochastic Gradient Descent beneficial?',
                        'answer' => 'Randomness in Stochastic Gradient Descent introduces deviations in the gradient calculation, helping to prevent the algorithm from quickly converging to a local optimum, potentially leading to a better overall solution.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How does the gradient calculated from a mini-batch differ from the ideal gradient?',
                        'answer' => 'The gradient calculated from a mini-batch may not perfectly represent the optimal gradient direction for the entire dataset, as it introduces random deviations due to the imperfect representation by the subset.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What are the impacts of batch size on gradient calculation?',
                        'answer' => 'Batch size impacts the calculation by balancing between computational efficiency and the ability to escape local minima; larger batches offer stable direction but may get stuck in local minima, while smaller batches introduce more randomness and flexibility.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'Why is Mini-batch Stochastic Gradient Descent preferred in practice?',
                        'answer' => 'Mini-batch Stochastic Gradient Descent is preferred due to its balanced approach, combining the efficiency and stability of batch processing with the randomness of SGD, making it well-suited for leveraging GPU parallel computations and handling large datasets.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What transforms a division problem into an unconstrained convex optimization problem in gradient-based algorithms?',
                        'answer' => 'The transformation involves minimizing the square of the difference between a given fraction and the product of a variable and a constant, effectively converting the division problem into optimizing a convex function.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What is the principle behind the gradient descent algorithm?',
                        'answer' => 'The principle is that by moving in the opposite direction to the gradient of the function, the value of the function decreases, aiming to find the minimum value of the function.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How is the gradient of a function represented and calculated?',
                        'answer' => 'The gradient is represented as a vector of the function’s partial derivatives with respect to its variables. It indicates how the function changes with respect to these variables and is calculated using the derivatives.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What role does the learning rate play in gradient descent?',
                        'answer' => 'The learning rate controls the scale of the gradient, determining the step size during the update process. It ensures a controlled and steady approach towards finding the optimal point, preventing overshooting and divergence.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How does gradient descent handle multivariate functions?',
                        'answer' => 'For multivariate functions, gradient descent calculates partial derivatives for each variable to form a gradient vector. The process updates each variable independently according to its partial gradient, optimizing the function collectively.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How do momentum and Adam algorithms improve gradient descent optimization?',
                        'answer' => 'Momentum and Adam improve gradient descent by integrating historical gradient records, facilitating smoother, more effective optimization. This approach mitigates fluctuations, enhances decision-making, and promotes faster convergence while reducing the risk of local optima.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What is the purpose of adding momentum to gradient descent?',
                        'answer' => 'Adding momentum to gradient descent incorporates past gradients into its calculations, akin to physical momentum, thus improving navigation through difficult areas like noisy or flat gradient regions by maintaining consistent direction over iterations, and achieving faster convergence.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How does learning rate decay schedules contribute to the optimization process?',
                        'answer' => 'Learning rate decay schedules adjust the learning rate during training, typically starting with a larger rate and gradually decreasing it. This strategic approach balances convergence speed and precision, avoids overshooting, enhances adaptability, and prevents premature convergence.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'Why is momentum effective in navigating complex optimization landscapes?',
                        'answer' => 'Momentum is effective because it smooths out noise by integrating past gradient data, reduces directional cancellations for steady progress, and adds inertia to effectively bypass shallow local minima. These mechanisms collectively aid in navigating complex optimization landscapes more adeptly.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What are the benefits of implementing learning rate decay in gradient descent algorithms?',
                        'answer' => 'The benefits include balanced convergence speed and precision, avoiding overshooting the minimum, enhanced adaptability in complex loss landscapes, and preventing premature convergence to sub-optimal points by adjusting the step size dynamically during training.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What is the definition of a loss surface in the context of machine learning?',
                        'answer' => 'The essence of a loss surface is to describe the relationship between the loss value and the model parameters, allowing the analysis of changes in loss value with respect to parameter modifications. It is defined based on the model’s output, the label, and the parameters.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How does visualization help in understanding loss surfaces and their properties like convexity?',
                        'answer' => 'Visualization of loss surfaces involves sampling parameters within a certain range and computing loss for each parameter combination, allowing insights into characteristics of loss functions, such as single concave regions or multiple optimal solutions, facilitating intuitive understanding of gradient descent algorithms.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'Why are convex and non-convex functions important in the optimization process of machine learning models?',
                        'answer' => 'Convex functions have a single minimum point, simplifying optimization as gradient descent algorithms converge to this global minimum. Non-convex functions present challenges with multiple local minima or saddle points, requiring advanced algorithm design and parameter initialization strategies to find suitable solutions.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What strategies are used to escape local minima during the optimization of neural networks?',
                        'answer' => 'Strategies include the momentum method to "jump over" local minima, adding perturbations during training, using different initialization strategies to change initial parameter values, and employing learning rate annealing to prevent premature convergence.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How do modern deep learning perspectives view the optimization challenges of local minima and saddle points?',
                        'answer' => 'Modern deep learning perspectives suggest that for large-scale neural networks, reaching a local minimum is often sufficient for high performance. Saddle points are seen as a more common challenge than local minima, with strategies like adaptive learning rate algorithms and higher-order optimization techniques used to navigate these issues.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How is optimization present in everyday life and provide examples?',
                        'answer' => 'Optimization guides route planning in navigation apps to find the quickest path, shapes online shopping experiences by comparing prices, manages energy efficiency through smart thermostats, optimizes airline schedules, tailors personal fitness plans, adjusts video quality for streaming services, curates personalized social media feeds, and streamlines manufacturing production and inventory.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What constitutes the formal structure of an optimization problem?',
                        'answer' => 'An optimization problem includes an objective function to be maximized or minimized, represented as f(x), where x are the decision variables. It also involves constraints, either as equations or inequalities, defining the feasible region for the solution. The objective is to find the optimal value of x that achieves the maximum or minimum of f(x) while satisfying the constraints.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What is the role of decision variables and constraints in optimization modeling, illustrated with a bakery production example?',
                        'answer' => 'Decision variables represent the controllable aspects of the problem, such as the number of products to produce. Constraints are the limitations, like resource availability, that the solution must satisfy. In a bakery example, decision variables could be the number of white and whole wheat bread loaves produced, while constraints could involve flour availability and oven capacity, aiming to maximize profit within these limits.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'Explain the concept of division as an optimization problem and its benefits.',
                        'answer' => 'Division can be viewed as an optimization problem where the objective is to minimize the difference between a given ratio and a product of an unknown quantity. This approach allows finding a solution without explicitly defining division, using iterative adjustments to improve the objective function\'s value, demonstrating the optimization process without direct division operations.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How does long division serve as a grid search optimization technique and what are its advantages and disadvantages?',
                        'answer' => 'Long division is analogous to a grid search optimization technique, methodically refining the quotient to minimize the remainder. Its advantages include simplicity and straightforwardness, suitable for hyperparameter tuning without complex analyses. However, its efficiency drastically decreases with higher dimensions due to the exponential growth of the search space, posing significant scalability issues.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What is the purpose of using a computational graph for neural network optimization?',
                        'answer' => 'A computational graph simplifies the analysis of a neural network by visually representing the operations and functions that process the input to produce an output. It helps in understanding the flow of data and gradients within the network, making it easier to explore different optimization challenges, such as finding optimal parameters to maximize or minimize the output based on given inputs.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How does introducing a loss function transform the optimization problem in neural network training?',
                        'answer' => 'Introducing a loss function, such as mean squared error, adds a quantitative measure to assess the difference between the predicted output and the actual label. It transforms the optimization problem by setting a new target variable—the loss value—and making the decision variables the network parameters. The objective becomes finding the optimal parameters that minimize the loss, effectively aligning the predicted output with the target labels.',
                        'difficulty' => 'Easy',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How do you calculate the gradient for a simplified neural network model?',
                        'answer' => 'To calculate the gradient for a simplified neural network model with a single input, output, and scalar parameters, we use the chain rule to break down the loss function into a series of operations and compute their derivatives step by step. This involves derivatives of the activation functions and the squared error loss function with respect to the weights, which is done by sequentially applying the chain rule to obtain the final derivatives of the loss with respect to each weight parameter.',
                        'difficulty' => 'Medium',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How is the gradient calculation extended for a dataset in neural network training?',
                        'answer' => 'The gradient calculation is extended for a dataset by computing the mean (or sum) of the losses over the entire dataset instead of optimizing the loss for a single input-output pair. This involves calculating the gradient for each individual sample first and then summing these gradients to determine the overall gradient. The process ensures that the optimization aims to minimize the average loss across all samples, accounting for the entire dataset in the training.',
                        'difficulty' => 'Medium',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],[
                        'topic_id' => $topicId,
                        'question' => 'What is a subgradient and why is it important in neural network optimization?',
                        'answer' => 'A subgradient extends the concept of a gradient to non-differentiable functions, offering a feasible direction for increasing function values. It is crucial in neural network optimization because it allows for the computation of gradients in areas where traditional gradients are undefined, such as with functions like ReLU or Max Pooling, enabling effective backpropagation and ensuring convergence of the final loss value.',
                        'difficulty' => 'Medium',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'How is the subgradient computed for functions that are non-differentiable at certain points?',
                        'answer' => 'For functions that are non-differentiable at certain points, a feasible direction that approximates the gradient is ascertained by exploring nearby regions. This approach allows for continued progress in optimization despite the traditional gradient being undefined. Subgradients can also be predefined to provide viable directions for non-differentiable points in advance, optimizing efficiently across the function\'s landscape.',
                        'difficulty' => 'Medium',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'Describe the concept of gradient routing and its importance in neural network optimization.',
                        'answer' => 'Gradient routing is a process in optimization where the gradient "flows" through a specific path based on the function\'s operation, such as maximum or sorting. It is essential in neural network optimization for handling functions whose output is a routing of the input, ensuring that gradients are correctly propagated back through the network. This allows for effective training even in the presence of non-differentiable operations.',
                        'difficulty' => 'Hard',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'Why are hash functions and certain iterative algorithms like EMD considered difficult to incorporate into gradient-based learning models?',
                        'answer' => 'Hash functions and certain iterative algorithms like Empirical Mode Decomposition (EMD) are challenging to incorporate into gradient-based learning models because they do not have well-defined gradients or subgradients. Hash functions, by design, are non-invertible and adding a gradient would undermine their security features. Similarly, algorithms like EMD involve variable mappings from inputs to outputs, making gradient computation complex due to discontinuities and unpredictable behavior.',
                        'difficulty' => 'Hard',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
                    [
                        'topic_id' => $topicId,
                        'question' => 'What are the challenges of incorporating non-differentiable functions into gradient-based optimization algorithms?',
                        'answer' => 'The challenges include the absence of well-defined gradients, making it difficult to compute the direction of steepest descent. This complicates the backpropagation process, as the network cannot learn from the non-differentiable function. Additionally, non-differentiable functions can introduce discontinuities, making it challenging to ensure convergence and stability in the optimization process.',
                        'difficulty' => 'Hard',
                        'type' => 'speech',
                        'created_at' => Carbon::now(),
                        'updated_at' => Carbon::now(),
                        'course_id' => $courseId,
                    ],
            ];

            // Insert questions for the MachineLearning course
            foreach ($questions as $question) {
                DB::table('questions')->insert([
                    'topic_id' => $question['topic_id'],
                    'question' => $question['question'],
                    'answer' => $question['answer'],
                    'image_url' => $question['image_url'] ?? null,
                    'video_id' => $question['video_id'] ?? null,
                    'difficulty' => $question['difficulty'],
                    'type' => $question['type'],
                    'created_at' => Carbon::now(),
                    'updated_at' => Carbon::now(),
                    'course_id' => $question['course_id'],
                ]);
            }
        }
    }
}
